{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6d4b7984497964"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# :author: AnnieKLamar"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8337d72c6e8bd7e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "def clean_files_to_df(quiet=True):\n",
    "    \"\"\"Reads all csv files in data to a pandas dataframe.\"\"\"\n",
    "    path = os.getcwd() + '\\data'\n",
    "    text_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    header = ['string']\n",
    "    all_lines = []\n",
    "    for file in text_files:\n",
    "        if not quiet:\n",
    "            print('File Name:', file.split(\"\\\\\")[-1])\n",
    "\n",
    "        # open and read file             \n",
    "        open_file = open(file, 'r', encoding='utf8')\n",
    "        file_lines = open_file.readlines()\n",
    "        \n",
    "        for line in file_lines:\n",
    "            if len(line.split(' '))>3:\n",
    "                line = line.replace(\"†\", \"\")\n",
    "                line = line.replace('\"\"', '')\n",
    "                line = line.replace('…', '')\n",
    "                line = line.replace(\"'\", \"\")\n",
    "                line = line.replace('”', \"\")\n",
    "                line = line.replace('“', '')\n",
    "                line = line.replace('—', '')\n",
    "                line = line.replace('῾', '')\n",
    "                line = line.replace('\"', '')\n",
    "                line = line.replace(\"!\", '')\n",
    "                line = line.replace('*', '')\n",
    "                line = line.replace('-', '')\n",
    "                line = line.replace('>', '')\n",
    "                all_lines.append([line.strip()])\n",
    "                \n",
    "    corpus_df = pd.DataFrame(all_lines, columns=header)\n",
    "    return corpus_df\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.last = 0\n",
    "        self.first_run = True\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        if self.first_run:\n",
    "            self.last = model.get_latest_training_loss()\n",
    "            self.first_run = False\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, self.last))\n",
    "            self.epoch += 1\n",
    "        else:\n",
    "            loss = model.get_latest_training_loss()\n",
    "            converted = loss - self.last\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, converted))\n",
    "            self.epoch += 1\n",
    "            self.last = loss\n",
    "            \n",
    "def preprocessing(corpus_df):\n",
    "    training_data = []\n",
    "    \n",
    "    for index,row in corpus_df.iterrows():\n",
    "        training_data.append(row['string'])\n",
    "    \n",
    "    tokenized = []\n",
    "    for item in training_data:\n",
    "        tokens = item.split(\" \")\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized\n",
    "\n",
    "def train_skipgram(training_data):\n",
    "    model = gensim.models.Word2Vec(epochs=100, sg=1, sentences=training_data, min_count=5, compute_loss=True, \n",
    "                                   callbacks=[callback()])\n",
    "    with tempfile.NamedTemporaryFile(prefix='gensim-sg-model-', delete=False) as tmp:\n",
    "        temporary_filepath = tmp.name\n",
    "        model.save(temporary_filepath)\n",
    "        print(\"Model saved: \" + temporary_filepath)\n",
    "    return model\n",
    "\n",
    "def train_cbow(training_data):\n",
    "    model = gensim.models.Word2Vec(epochs=175, sg=0, sentences=training_data, min_count=5, compute_loss=True, \n",
    "                                   callbacks=[callback()])\n",
    "    with tempfile.NamedTemporaryFile(prefix='gensim-cbow-model-', delete=False) as tmp:\n",
    "        temporary_filepath = tmp.name\n",
    "        model.save(temporary_filepath)\n",
    "        print(\"Model saved: \" + temporary_filepath)\n",
    "    return model\n",
    "\n",
    "def get_ten_closest(model, output_file):\n",
    "    closest_file = open(output_file, 'w+', encoding='utf8', newline='')\n",
    "    options_csv_writer = csv.writer(closest_file)\n",
    "    options_fields = ['word', 'sim01word', 'sim01score', 'sim02word', 'sim02score', 'sim03word', 'sim03score',\n",
    "                      'sim04word', 'sim04score', 'sim05word', 'sim05score', 'sim06word', 'sim06score',\n",
    "                      'sim07word', 'sim07score', 'sim08word', 'sim08score', 'sim09word', 'sim09score', \n",
    "                      'sim10word', 'sim10score']\n",
    "    options_csv_writer.writerow(options_fields)\n",
    "    for key in model.wv.key_to_index.keys():\n",
    "        topten = model.wv.most_similar(positive=[key], topn=10)\n",
    "        row = [key]\n",
    "        for pair in topten:\n",
    "            row.append(pair[0])\n",
    "            row.append(pair[1])\n",
    "        options_csv_writer.writerow(row)\n",
    "        \n",
    "def get_ten_farthest(model, output_file):\n",
    "    closest_file = open(output_file, 'w+', encoding='utf8', newline='')\n",
    "    options_csv_writer = csv.writer(closest_file)\n",
    "    options_fields = ['word', 'neg01word', 'neg01score', 'neg02word', 'neg02score', 'neg03word', 'neg03score',\n",
    "                      'neg04word', 'neg04score', 'neg05word', 'neg05score', 'neg06word', 'neg06score',\n",
    "                      'neg07word', 'neg07score', 'neg08word', 'neg08score', 'neg09word', 'neg09score', \n",
    "                      'neg10word', 'neg10score']\n",
    "    options_csv_writer.writerow(options_fields)\n",
    "    for key in model.wv.key_to_index.keys():\n",
    "        topten = model.wv.most_similar(negative=[key], topn=10)\n",
    "        row = [key]\n",
    "        for pair in topten:\n",
    "            row.append(pair[0])\n",
    "            row.append(pair[1])\n",
    "        options_csv_writer.writerow(row)       \n",
    "\n",
    "def cbow_pipeline():\n",
    "    print(\"Training CBOW model...\")\n",
    "    training_data = preprocessing(clean_files_to_df())\n",
    "    cbow_model = train_cbow(training_data)\n",
    "    get_ten_closest(cbow_model, 'results/cbow_most_similar.csv')\n",
    "    get_ten_farthest(cbow_model, 'results/cbow_least_similar.csv')\n",
    "    print(\"Top ten files saved as csv.\")\n",
    "    \n",
    "def skipgram_pipeline():\n",
    "    print(\"Training Skipgram model...\")\n",
    "    training_data = preprocessing(clean_files_to_df())\n",
    "    sg_model = train_skipgram(training_data)\n",
    "    get_ten_closest(sg_model, 'results/sg_most_similar.csv')\n",
    "    get_ten_farthest(sg_model, 'results/sg_least_similar.csv')\n",
    "    print(\"Top ten files saved as csv.\")\n",
    "    \n",
    "cbow_pipeline()\n",
    "skipgram_pipeline()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
